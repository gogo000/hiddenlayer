{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HiddenLayer Graph Demo - PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models\n",
    "import hiddenlayer as hl"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 with BatchNorm\n",
    "model = torchvision.models.vgg16()\n",
    "\n",
    "# Build HiddenLayer graph\n",
    "# Jupyter Notebook renders it automatically\n",
    "hl.build_graph(model, torch.zeros([1, 3, 224, 224]))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexNet\n",
    "model = torchvision.models.alexnet()\n",
    "\n",
    "# Build HiddenLayer graph\n",
    "hl_graph = hl.build_graph(model, torch.zeros([1, 3, 224, 224]))\n",
    "\n",
    "# Use a different color theme\n",
    "hl_graph.theme = hl.graph.THEMES[\"blue\"].copy()  # Two options: basic and blue\n",
    "hl_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforms and Graph Expressions\n",
    "\n",
    "A Graph Expression is like a Regular Expression for graphs. It simplifies searching for nodes that fit a particular pattern. For example, the graph expression `Conv > Relu` will find Conv layers that are followed by RELU layers. And the expressions `Conv | MaxPool` will match any Conv and MaxPool layers that are in parallel branches (i.e. have the same parent node). See examples of more complex graph expressions below.\n",
    "\n",
    "Once the graph expression finds the nodes, we use Transforms to modify them. For example, if we want to delete all nodes of type `Const`, we'll use the transform `Prune(\"Const\")`. The graph expression here is simple, `Const`, which matches any node with operation of type Const. And the Prune() transform deletes the node.\n",
    "\n",
    "See more examples below. And, also, check `SIMPLICITY_TRANSFORMS` in `transforms.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ResNet101\n",
    "\n",
    "ResNet101 is a large network, but it consists of repetitive patterns. To simplify drawing the graph, we define our own transforms that find Residual Blocks and folds them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet101\n",
    "model = torchvision.models.resnet101()\n",
    "\n",
    "# Rather than using the default transforms, build custom ones to group\n",
    "# nodes of residual and bottleneck blocks.\n",
    "transforms = [\n",
    "    # Fold Conv, BN, RELU layers into one\n",
    "    hl.transforms.Fold(\"Conv > BatchNorm > Relu\", \"ConvBnRelu\"),\n",
    "    # Fold Conv, BN layers together\n",
    "    hl.transforms.Fold(\"Conv > BatchNorm\", \"ConvBn\"),\n",
    "    # Fold bottleneck blocks\n",
    "    hl.transforms.Fold(\"\"\"\n",
    "        ((ConvBnRelu > ConvBnRelu > ConvBn) | ConvBn) > Add > Relu\n",
    "        \"\"\", \"BottleneckBlock\", \"Bottleneck Block\"),\n",
    "    # Fold residual blocks\n",
    "    hl.transforms.Fold(\"\"\"ConvBnRelu > ConvBnRelu > ConvBn > Add > Relu\"\"\",\n",
    "                       \"ResBlock\", \"Residual Block\"),\n",
    "    # Fold repeated blocks\n",
    "    hl.transforms.FoldDuplicates(),\n",
    "]\n",
    "\n",
    "# Display graph using the transforms above\n",
    "hl.build_graph(model, torch.zeros([1, 3, 224, 224]), transforms=transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comment out one of these lines if you want to see the original graph before transforms are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without simplification transforms, but include framework transforms\n",
    "# Framework transforms map PyTorch or TensorFlow graphs into a standard format\n",
    "# based, mostly, on ONNX naming conventions.\n",
    "\n",
    "# hl.build_graph(model, torch.zeros([1, 3, 224, 224]), transforms=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all transforms completely\n",
    "# Override both, simplicity transforms and framework transforms, to get\n",
    "# the original raw graph.\n",
    "\n",
    "# hl.build_graph(model, torch.zeros([1, 3, 224, 224]), transforms=[], framework_transforms=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}